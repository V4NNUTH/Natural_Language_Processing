{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da932dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['james_joyce_ulysses.txt',\n",
       " 'moby_dick_melville.txt',\n",
       " 'night_and_day_virginia_woolf.txt',\n",
       " 'robinson_crusoe_defoe.txt',\n",
       " 'sons_and_lovers_lawrence.txt',\n",
       " 'the_way_of_all_flash_butler.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.listdir(\"books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f85f5d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "def text2paragraphs(filename, min_size=1):\n",
    "    #A Text contained in the files 'filename' will be read\n",
    "    # and chopped into paragraph. Paragraph with a string\n",
    "    #length less than min_size will be ignored.\n",
    "    #A list of paragraph string will be returned.\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        txt = f.read()\n",
    "    paragraphs = [para for para in txt.split(\"\\n\\n\") if len(para) > min_size]\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "760cf12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Virginia Woolf', 'Samuel Butler', 'Herman Melville',\n",
    "        'David Herbert Lawrence', 'Daniel Defoe', 'James Joyce']\n",
    "path = \"books/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eca754a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ja', 'mo', 'ni', 'ro', 'so', 'th']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(\"books\")\n",
    "labels = {fname[:2] for fname in files if fname.endswith(\".txt\")}\n",
    "labels = sorted(list(labels))\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f38a68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['james_joyce_ulysses.txt', 'moby_dick_melville.txt', 'night_and_day_virginia_woolf.txt', 'robinson_crusoe_defoe.txt', 'sons_and_lovers_lawrence.txt', 'the_way_of_all_flash_butler.txt']\n"
     ]
    }
   ],
   "source": [
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59b3a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "targets = []\n",
    "for fname in files:\n",
    "    if fname.endswith(\".txt\"):\n",
    "        paras = text2paragraphs(path + fname, min_size=150)\n",
    "        data.extend(paras)\n",
    "        country = fname[:2]\n",
    "        index = labels.index(country)\n",
    "        targets += [index] * len(paras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47fe4b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "data_targets = list(zip(data, targets))\n",
    "# create random permuation on list:\n",
    "data_targets = random.sample(data_targets, len(data_targets))\n",
    "data, targets = list(zip(*data_targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87e195dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "data_targets = list(zip(data, targets))\n",
    "# create random permuation on list:\n",
    "data_targets = random.sample(data_targets, len(data_targets))\n",
    "data, targets = list(zip(*data_targets))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "res = train_test_split(data, targets,\n",
    "                        train_size=0.8,\n",
    "                        test_size=0.2,\n",
    "                        random_state=42)\n",
    "train_data, test_data, train_targets, test_targets = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d7bb069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.914534567229178\n",
      "F1-score:  0.906989407012656\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=list(ENGLISH_STOP_WORDS))\n",
    "#vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(train_data)\n",
    "\n",
    "# creating a classifier\n",
    "classifier = MultinomialNB(alpha=.01)\n",
    "classifier.fit(vectors, train_targets)\n",
    "\n",
    "vectors_test = vectorizer.transform(test_data)\n",
    "predictions = classifier.predict(vectors_test)\n",
    "accuracy_score = metrics.accuracy_score(test_targets,predictions)\n",
    "f1_score = metrics.f1_score(test_targets,\n",
    "                            predictions,\n",
    "                            average='macro')\n",
    "print(\"accuracy score: \", accuracy_score)\n",
    "print(\"F1-score: \", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccb79160",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_texts = [\"Es ist nicht von Bedeutung, wie langsam du gehst, solange du nicht stehenbleibst.\",\n",
    "            \"Man muss das Unmögliche versuchen, um das Möglichezu erreichen.\",\n",
    "            \"It's so much darker when a light goes out than it would have been if it had never shone.\",\n",
    "            \"Rien n'est jamais fini, il suffit d'un peu de bonheur pour que tout recommence.\",\n",
    "            \"Girano le stelle nella notte ed io ti penso forte forte e forte ti vorrei\"]\n",
    "\n",
    "sources = [\"Konfuzius\", \"Hermann Hesse\", \"John Steinbeck\", \"EmileZola\", \"Gianna Nannini\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fbec518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ja\n",
      "1 mo\n",
      "4 so\n",
      "0 ja\n",
      "0 ja\n"
     ]
    }
   ],
   "source": [
    "vtest = vectorizer.transform(some_texts)\n",
    "predictions = classifier.predict(vtest)\n",
    "for label in predictions:\n",
    "    print(label, labels[label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed858814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
